@misc{HAWHamburgMARS,
author = {{HAW Hamburg}},
title = {{- MARS Group}},
howpublished="[Online] \url{https://mars-group.org/}",
urldate = {2018-02-08}
}

@article{Lin2017,
abstract = {Web archiving initiatives around the world capture ephemeralWeb content to preserve our collective digital memory. How- ever, unlocking the potential of Web archives for humanities scholars and social scientists requires a scalable analytics in- frastructure to support exploration of captured content.We presentWarcbase, an open-sourceWeb archiving platform that aims to fill this need.Our platform takes advantage of modern open-source “big data” infrastructure, namely Hadoop,HBase, and Spark, that has been widely deployed in industry.Warcbase provides two main capabilities: support for temporal brows- ing and a domain-specific language that allows scholars to interrogate Web archives in several different ways. This work represents a collaboration between computer scientists and historians, where we have engaged in iterative codesign to build tools for scholars with no formal computer science training. To provide guidance, we propose a process model for scholarly interactions withWeb archives that begins with a question and proceeds iteratively through four main steps: filter, analyze, aggregate, and visualize.We call this the FAAV cycle for short and illustrate with three prototypical case studies. This article presents the current state of the project and discusses future directions.},
author = {Lin, Jimmy and Milligan, Ian and Wiebe, Jeremy and Zhou, Alice},
doi = {10.1145/3097570},
file = {:Users/prannoy/Thesis/Research/WARCBase.pdf:pdf},
issn = {15564711},
journal = {Journal on Computing and Cultural Heritage},
number = {4},
pages = {1--30},
title = {{Warcbase: Scalable analytics infrastructure for exploring web archives}},
howpublished = "[Online] \url{http://dl.acm.org/citation.cfm?doid=3129537.3097570}",
volume = {10},
year = {2017},
note={{ISBN:} 15564711}
}


@article{Format2016,
author = {Format, File and Model, Data and Software, Third Party and Model, Programming},
file = {:Users/prannoy/Thesis/Research/HDF5Intro.pdf:pdf},
pages = {1--25},
title = {{High Level Introduction to HDF5 Introduction to HDF5}},
year = {2016}
}

@article{Torre2017,
author = {Torre, Cesar De La and Wagner, Bill and Rousos, Mike},
file = {:Users/prannoy/Thesis/Research/NET-Microservices-Architecture-for-Containerized-NET-Applications-(Microsoft-eBook).pdf:pdf},
title = {{.NET Microservices: Architecture for Containerized .NET Apllications}},
year = {2017}
}

@article{Heber2012,
abstract = {In this document, we specify a REST [Fielding2000] interface for HDF5 data stores. We describe HDF5 resources, URIs, and resource representations, and show a simple example of how to use this interface to populate an HDF5 store.},
author = {Heber, Gerd},
file = {:Users/prannoy/Thesis/Research/RESTful{\_}HDF5.pdf:pdf},
pages = {1--43},
title = {{RESTful HDF5}},
howpublished = "[Online] \url{http://www.hdfgroup.org/pubs/papers/RESTful{\_}HDF5.pdf}",
year = {2012}
}

@article{Folk2010,
abstract = {Fifteen years ago, The HDF Group set out to re-invent the HDF format and software suite to address two conflicting challenges. The first was to enable exceptionally scalable, extensible storage and access for every kind of scientific and engineering data. The second was to facilitate access to data stored in the HDF long into the future. This challenge grew out of necessity. Some of the most ambitious scientific projects, such as NASA's Earth Observing System, need scalable solutions to their data generation and data gathering activities. At the same time, data consumers in these projects need assurances that their data will retain its value and accessibility for decades to centuries into the future. The HDF Group has worked to discover and pursue technological and institutional strategies that address these requirements for the broadest possible range of data applications. To achieve this objective, care and resources must be applied in the design, development, and maintenance of the technologies, and attention must be paid to integration with complementary technologies. This technical rigor must be complemented by an institutional model that will provide resources for current activities and sustainability for the long term, as well as active involvement with data producers and consumers to understand and respond to their needs. The paper describes how The HDF Group balances its commitment to providing the best solutions to today's data challenges against the need to meet data preservation requirements.},
author = {Folk, Mike and Pourmal, Elena},
doi = {10.1145/2039274.2039285},
file = {:Users/prannoy/Thesis/Research/HDF5Performance.pdf:pdf},
isbn = {9781450301091},
journal = {Proceedings of the 2010 Roadmap for Digital Preservation Interoperability Framework Workshop on - US-DPIF '10},
keywords = {hdf,hdf4,hdf5,long-term data preservation,open source},
pages = {1--8},
title = {{Balancing performance and preservation lessons learned with HDF5}},
howpublished = "[Online] \url{http://dl.acm.org/citation.cfm?doid=2039274.2039285}",
year = {2010},
note={{ISBN:} 9781450301091}
}

@article{Savic2007,
abstract = {In the field of simulations storing and exchanging simulation data are important tasks. The quantity of simulation data can be rather big and at the same time this data can appear in different formats. The conversion of big quantities of data can be extremely time-consuming. In this article we focus on simulations in telecommunications. Therefore we have studied the needs of the telecommunication community and defined a reference model of a simulation process. According to the needs we have developed a software tool CostGlue, which represents a central repository for data produced with different types of simulation tools and acts as a converter of different output formats into different input formats. CostGlue has modular software architecture. This enables further development and contributions from any other research sphere of activity. The core of the software tool represents the application CoreGlue responsible for communicating with the database. CoreGlue represents unified interface for writing to the database and reading from it. Specific functions like import and export of data and different mathematical calculations are represented as a set of self-described modules, which are loaded as necessary. The graphic user interface is introduced as a web application for the simplicity of use and effective remote access to the application. The software package CostGlue is going to be released as free software with the possibility of further development.},
author = {Savic, Dragan and Potorti, Francesco and Furfari, Francesco and Pustiek, Matevz and Beter, Janez and Tomazic, Sao},
doi = {10.1007/978-0-387-73908-3_23},
file = {:Users/prannoy/Thesis/Research/HDF5ForSimulations.pdf:pdf},
isbn = {978-0-387-73907-6},
journal = {Recent Advances in Modeling and Simulation Tools for Communication Networks and Services},
keywords = {Computer simulation;Computer software;},
pages = {443--462},
title = {{A tool for packaging and exchanging simulation results}},
year = {2007},
note={{ISBN:} 978-0-387-73907-6}
}

@article{Heber2014,
abstract = {The purpose of this document is to offer some guidance on the use of HDF5 with Hadoop. If you are puzzled by the relationship between HDF and Hadoop, welcome to the club! We have condensed the acme of confusion into a few questions for which one might expect straightforward answers. In the discussion, we present our answers at three levels of intimidation: a one phrase answer (in the subtitle), a multi-paragraph elaboration, and a set of appendices. If you are not satisfied after the third level, please, send us your answer! And don't hesitate to send us your questions either.},
author = {Heber, Gerd and Folk, Mike and Koziol, Quincey A},
file = {:Users/prannoy/Thesis/Research/Big{\_}HDF{\_}FAQs.pdf:pdf},
pages = {1--32},
title = {{Big HDF FAQs - Everything that HDF Users have Always Wanted to Know about Hadoop . . . But Were Ashamed to Ask}},
year = {2014}
}

@article{Holzmann2016,
abstract = {Web archives are a valuable resource for researchers of vari-ous disciplines. However, to use them as a scholarly source, researchers require a tool that provides efficient access to Web archive data for extraction and derivation of smaller datasets. Besides efficient access we identify five other ob-jectives based on practical researcher needs such as ease of use, extensibility and reusability. Towards these objectives we propose ArchiveSpark, a framework for efficient, distributed Web archive processing that builds a research corpus by working on existing and standardized data formats commonly held by Web archiv-ing institutions. Performance optimizations in ArchiveS-park, facilitated by the use of a widely available metadata index, result in significant speed-ups of data processing. Our benchmarks show that ArchiveSpark is faster than alterna-tive approaches without depending on any additional data stores while improving usability by seamlessly integrating queries and derivations with external tools.},
archivePrefix = {arXiv},
arxivId = {arXiv:1702.01015v1},
author = {Holzmann, Helge and Goel, Vinay and Anand, Avishek},
doi = {10.1145/2910896.2910902},
eprint = {arXiv:1702.01015v1},
file = {:Users/prannoy/Thesis/Research/ActiveSpark.pdf:pdf},
isbn = {978-1-4503-4229-2},
issn = {15525996},
journal = {Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries},
keywords = {big data,data extraction,web archives},
pages = {83--92},
title = {{ArchiveSpark: Efficient Web Archive Access, Extraction and Derivation}},
url = {http://doi.acm.org/10.1145/2910896.2910902},
year = {2016},
note={{ISBN:} 978-1-4503-4229-2}
}

@article{Heber2014a,
abstract = {The purpose of this document is to offer some guidance on the use of HDF5 with Hadoop. If you are puzzled by the relationship between HDF and Hadoop, welcome to the club! We have condensed the acme of confusion into a few questions for which one might expect straightforward answers. In the discussion, we present our answers at three levels of intimidation: a one phrase answer (in the subtitle), a multi-paragraph elaboration, and a set of appendices. If you are not satisfied after the third level, please, send us your answer! And don't hesitate to send us your questions either. We would like to thank all advocates, developers, maintainers, and supporters for their contributions to open-source software including HDF5 and Apache Hadoop.},
author = {Heber, Gerd and Group, The H D F and Folk, Mike and Group, The H D F and Koziol, Quincey A and Group, The H D F},
file = {:Users/prannoy/Thesis/Research/HadoopVsHDF5.pdf:pdf},
pages = {1--32},
title = {{Big HDF FAQs}},
howpublished = "[Online] \url{https://support.hdfgroup.org/pubs/papers/Big{\_}HDF{\_}FAQs.pdf}",
year = {2014}
}

@book{Newman2015,
abstract = {Distributed systems have become more fine-grained in the past 10 years, shifting from code-heavy monolithic applications to smaller, self-contained microservices. But developing these systems brings its own set of headaches. With lots of examples and practical advice, this book takes a holistic view of the topics that system architects and administrators must consider when building, managing, and evolving microservice architectures.Microservice technologies are moving quickly. Author Sam Newman provides you with a firm grounding in the concepts while diving into current solutions for modeling, integrating, testing, deploying, and monitoring your own autonomous services. You'll follow a fictional company throughout the book to learn how building a microservice architecture affects a single domain.Discover how microservices allow you to align your system design with your organization's goalsLearn options for integrating a service with the rest of your systemTake an incremental approach when splitting monolithic codebasesDeploy individual microservices through continuous integrationExamine the complexities of testing and monitoring distributed servicesManage security with user-to-service and service-to-service modelsUnderstand the challenges of scaling microservice architectures},
archivePrefix = {arXiv},
arxivId = {1606.04036},
author = {Newman, Sam},
booktitle = {O'Reilly},
doi = {10.1109/MS.2016.64},
eprint = {1606.04036},
file = {:Users/prannoy/Library/Application Support/Mendeley Desktop/Downloaded/Newman - 2015 - Building Microservices.pdf:pdf},
isbn = {978-1-491-95035-7},
issn = {07407459},
keywords = {www.it-ebooks.info},
pages = {280},
publisher = {Oreilly},
title = {{Building Microservices}},
howpublished = "[Online] \url{http://zempirians.com/ebooks/Sam Newman-Building Microservices-O{\%}27Reilly Media (2015).pdf https://www.google.hr/books?hl=en{\&}lr={\&}id=jjl4BgAAQBAJ{\&}pgis=1{\%}5Cnhttp:// oreilly.com/catalog/errata.csp?isbn=9781491950357 }",
year = {2015},
note={{ISBN:} 978-1-491-95035-7}
}


@article{SEV,
author = {Li, Xunjie},
file = {::},
title = {{SEV: a Storage-Efficient Versioning File System}},
howpublished = "[Online] \url{http://web.mit.edu/6.033/2013/wwwdocs/writing-samples/xunjieli.pdf}",
year = {2013}
}

@misc{Synology,
title = {{DiskStation Manager | Synology Inc.}},
howpublished = "[Online] \url{https://www.synology.com/en-global/dsm}",
urldate = {2018-02-09}
}

@book{DistributedSystems,
author = {{Van Steen}, Maarten and Tanenbaum, Andrew S},
file = {::},
isbn = {978-90-815406-2-9},
number = {3},
pages = {596},
title = {{Distributed Systems}},
url = {https://distsys.utwente.nl/Books/DS3/copies/180211/c5121ea7694d5a990b1d66618c2f86c8/Distributed{\_}Systems{\_}3-180211.pdf},
year = {2017},
note={{ISBN:} 978-90-815406-2-9}
}


@book{DDD,
abstract = {The most complicated aspect of large software projects is not the implementation, it is the real world domain that the software serves. Domain Driven Design is a vision and approach for dealing with highly complex domains that is based on making the domain itself the main focus of the project, and maintaining a software model that reflects a deep understanding of the domain. The vision was brought to the world by Eric Evans in his book "Domain Driven Design". Eric's work was based on 20 years of widely accepted best practices in the object community, as well as Eric's own insights. Domain Driven Design Quickly is a short, quick-readable summary and introduction to the fundamentals of DDD. A special interview with Eric Evans on the state of Domain Driven Design is also included.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Eric Evans},
booktitle = {Online},
doi = {978-0321125217},
eprint = {arXiv:1011.1669v3},
file = {:Users/prannoy/Thesis/Research/Eric Evans 2003 - Domain-Driven Design - Tackling Complexity in the Heart of Software.pdf:pdf},
isbn = "978-0-321-12521-7",
issn = {00155713},
pages = {104},
pmid = {12566760},
title = {{Domain Driven Design Quickly}},
year = {2006},
note={{ISBN:} 978-0-321-12521-7}
}

@book{SOA,
author = {Erl, Thomas and Merson, Paulo and Stoffers, Roger},
editor = {Wiegand, Greg},
file = {:Users/prannoy/Downloads/service-oriented-architecture-thomas-erl301(www.ebook-dl.com)/service-oriented-architecture-thomas-erl301(www.ebook-dl.com).pdf:pdf},
isbn = {978-0-13-385858-7},
pages = {428},
publisher = {Taub Mark},
title = {{Service-Oriented Architecture}},
year = {2016},
note={{ISBN:} 978-0-13-385858-7}
}
