\section{Unit Testing}
These tests are designed to verify the individual components of the software which validates that each unit of the software performs as intended. A White Box
Testing methodology has been applied for the unit testing. This method is used as the internal structure/requirements are known before hand. In this process,
the specific inputs and outputs are predetermined and the tests are ran to verify if the expected output is produced by that input. 

\par
Figure \ref{fig:CIbuild} above depicts
unit testing as an mandatory step for deployment of the Archive service. Running this test with every build process will boost the confidence 
in changing and maintaining the code for the Archive service. The ASP .NET platform offers a lot of unit-testing frameworks 
(e.g. XUnit, Nunit, MSTest/Visual Studio). Nunit is choosen as the preferred the testing framework for this service as it has a good reputation for
fast testing and it is already integrated the IDE i.e. Rider (JetBrains) used for this work. 

\par
Making stubs, and mocks/fake objects are one of the important steps to do a proper unit testing. This statement can be backed up saying that unit tests are
used to analyze only one module/method. To check only one module the other dependent objects have to faked. The Archive service is designed following the
SOLID \cite{Hotop2015} principle where the D stands for \textbf{Dependency Inversion Principle} where classes depend upon abstractions i.e. Interfaces not concrete
implementation. The application of this principle has allowed an easy method to fake objects as the depending classes are just abstractions and the concrete 
implementations are injected by the ASP .NET Dependency Injection framework. A framework name as Moq \footnote{https://github.com/Moq/moq4/wiki/Quickstart} is used
which can be used to fake objects i.e.Interfaces and return the desired output from them. 

\begin{lstlisting}[language={[Sharp]C}, caption={Example of faking objects using Moq framework}, captionpos=b,label={lst:moqEg}]
[Fact]
public async Task Archive_CorrectDataId_NoExceptions()
{
    // Arrange
    var fileRepoMock = new Mock<IFileRepo>();
    var fileClientMock = new Mock<IFileClient>();
    // Injecting the mock interfaces 
    var archiveFile = new ArchiveFile(fileRepoMock.Object, fileClientMock.Object);
    var testFileName = "test.zip";
    var testDataId = "4a97bd47-7713-4f4d-89c4-aacf04ae5d20";
    
    // Act
    // Using the Moq framework to return desired output to the method Archive
    fileRepoMock.Setup(m => m.AddFileForProject(It.IsAny<string>(), It.IsAny<byte[]>())).Returns(Task.CompletedTask);
    fileClientMock.Setup(m => m.GetFileAsync(It.IsAny<string>())).ReturnsAsync(new byte[]{1});

    Exception ex = null;
    try
    {
        await archiveFile.Archive(testFileName, testDataId);
    }
    catch (Exception e)
    {
        ex = e;
    }
    
    // Assert
    Assert.Null(ex);
}
\end{lstlisting}

Listing~\ref{lst:moqEg} illustrates an example of an unit test which is done on the Archive method for a file. The Moq framework injects the dependencies and in the setup method
one can return the desired output from the methods. This unit tests verifies if all the dependent methods work as expected.
Following the functional requirements mentioned in Section \ref{section:functionalReq} the unit tests are designed to test if the core logic functions as expected which includes error handling as well.

\subsection{Archive and Retrieve process test}
The unit test designed for these processes are aimed to test the components responsible for each type of resource (See Table \ref{table: archivedMars}) and its 
dependent module. The class overview can be seen from Figure \ref{fig:archiveClassDiagram} and \ref{fig:restoreClass}. Although, the figure does not show all the classes that the archive process is using
but when one sees the attributes that a class is using it will give an good idea of the remaining classes under use.

\subsubsection{Test if an background job is enqueued}
For this test, after a successful API request for archiving a project the module should create a background job using Hangfire. This test would check if the
background tasks were created or not. As it is a requirement for the archive process to run a background job so it must be ensured that a background job is created 
after a successful request. Listing \ref{lst:hangfireCreate} shows how the test was written to verify the creation of the background job.

\begin{lstlisting}[language={[Sharp]C}, caption={Hangfire Job creation test}, captionpos=b,label={lst:hangfireCreate}]
[Fact]
public async Task ArchiveProject_SucessfulRun_ShouldBeEnqueued()
{
    // Arrange
    var client = new Mock<IBackgroundJobClient>();
    var iArchiveRepoMock = new Mock<IArchiveStatusLoggerRepo>();
    var mockMarkProject = new Mock<IMarkProject>();
    var mockStatusHandler = new Mock<IBackgroundHandler>();
    var controller = new ArchiveApiController(client.Object, iArchiveRepoMock.Object, mockMarkProject.Object, mockStatusHandler.Object);
    var projectId = Guid.NewGuid().ToString();
    var model = new ArchiveRetrieveStatusModel(){Status = "FINISHED"}; 
    
    // Act
    iArchiveRepoMock.Setup(mo => mo.AddInitialStatus(model)).Returns(Task.CompletedTask);
    mockMarkProject.Setup(mark => mark.MarkAllResourcesForProjectAndGetMarkSession(It.IsAny<string>()))
        .Returns(Task.FromResult(Guid.NewGuid().ToString()));
    iArchiveRepoMock.Setup(mo => mo.GetStatusForId(projectId)).ThrowsAsync(new ResourceNotFoundException(It.IsAny<string>()));

    // Assert
    await controller.ArchiveProject(projectId);
    client.Verify(x => x.Create(
        It.Is<Job>(job => job.Method.Name == "Archive" && (string) job.Args[0] == projectId && job.Args[1] == JobCancellationToken.Null),
        It.IsAny<EnqueuedState>()));

}
\end{lstlisting}

\subsubsection{Return Conflict Message in Case a Process for the Project is Running}
For this test, it checks the denial of an archive or retrieve job creation when it is in "PROCESSING" state. It is of high priority that this check
for conflict works as expected otherwise there could be cases of unwanted data loss. 

\subsubsection{Exception Catch}
For this test, different checks are made in many modules if the expected exception is caught in case of an error. Also included is a check for the general
exception class as well. This test will ensure that some method will not just consume an exception when an unexpected error occurs. It is important because if 
the exceptions are not caught it would be very hard to fix bugs if some are introduced in the near future.

\newpage
\begin{lstlisting}[language={[Sharp]C}, caption={Exception catch test example}, captionpos=b,label={lst:exceptionCatch}]
[Fact]
public async Task MarkAllResourceAndGetSessionId_MarkingSvcClientReturnEmpty_ThrowsMarkingFailedException()
{
    // Arrange
    var markingSvcClientMock = new Mock<IMarkingSvcClient>();
    var iArchiveRepoMock = new Mock<IArchiveStatusLoggerRepo>();
    var markProject = new MarkProject(markingSvcClientMock.Object, iArchiveRepoMock.Object);
    
    // Act 
    markingSvcClientMock.Setup(mock => mock.MarkAndGetProjectResources(It.IsAny<string>())).ReturnsAsync(string.Empty);
    
    // Assert
    await Assert.ThrowsAnyAsync<MarkingFailedException>(async () =>
        await markProject.MarkAllResourcesForProjectAndGetMarkSession(It.IsAny<string>()));
}
\end{lstlisting}

\subsubsection{Expected Value Checks}
For this test, different modules are tested with a mock inputs (e.g. data id, mock models) and then the method under test is executed with an 
expected value. These tests are designed to ensure that the methods deliver correct results when a correct input from other modules are received.
These tests would aid to discover a faulty implementation according to the requirements mentioned for this work.

\subsubsection{Null or Empty Checks}
For this test, the different modules are tested against the most famous Null pointer exception in the different modules and how it is handled. This is an important
test case as the archive/retrieve process should still run and terminate accordingly if some resources are null. As an example, during a normal archive run if the result received from
the scenario service  is empty i.e. no scenarios in the project, the archive process should not fail due to a null pointer exception rather finish the archive process
as there are no children resources (See Figure \ref{fig:marsDependency}). Listing \ref{lst:scenarisEmpty} shows an example if the scenario client returns a null 
the process does not break but instead the method for archiving scenarios is not triggered. 

\begin{lstlisting}[language={[Sharp]C}, caption={No scenarios added if resource empty}, captionpos=b,label={lst:scenarisEmpty}]
[Fact]
public async Task Archive_NoScenarios_ScenariosNotAdded()
{
    // Arrange
    var scenarioRepoMock = new Mock<IScenarioRepo>();
    var scenarioClientMock = new Mock<IScenarioClient>();
    var markProjectMock = new Mock<IMarkProject>();
    var archiveScenario = new ArchiveScenario(scenarioRepoMock.Object, scenarioClientMock.Object, markProjectMock.Object);
    
    // Act
    markProjectMock.Setup(m => m.GetModelsForResourceType(It.IsAny<string>()))
        .Returns(DependantResourceMock.GetFilteResourceModels("scenario"));
    scenarioClientMock.Setup(m => m.GetScenariosById(It.IsAny<string>()))
        .ReturnsAsync(null);
    await archiveScenario.Archive(It.IsAny<string>());                
    
    // Assert
    scenarioRepoMock.Verify(m => m.AddProjectScenarios(It.IsAny<string>()), Times.Never);
}
\end{lstlisting}

\subsubsection{File Read Tests}
For this test, the module for reading files from the Synology is tested. To do so, dummy files were added (mocking the existence of Synology without 
a network connection) in the build for the unit tests. This test reads the file
and then compares it with the expected result. This test is designed to check if the file reading algorithm works as expected. Unfortunately, the module to write the data
could not be verified because when the tests are built they are packed into a .dll file where a write operation is not allowed.

\subsection{Test Coverage}
A total of 174 unit test cases were made on the different modules existing in the Archive service at the time of writing this thesis. The total time for running all 
the test cases measured on average is 1m50s. For a proper calculation of the Test coverage by available features in the service Table \ref{table:unitTestCov} gives an short
overview of the type of module and the different features available.

\begin{longtable}{|p{3cm}|p{10cm}|}
    \hline
        \textbf{Modules} & \textbf{Features}\\
    \hline
        Http communication client & File service client, Metadata service client, Scenario service client, Result configuration client, simulation plans client, 
        simulation runs client and simulation results client.\\    
    \hline
        API controller & Archive, Retrieve and background jobs.\\    
    \hline
        Archive & Files, Metadata, Scenario, Result configuration, simulation plans, simulation runs and simulation results.\\  
    \hline
        Retrieve & Files, Metadata, Scenario, Result configuration, simulation plans, simulation runs and simulation results.\\ 
    \hline
        File repositories & Read and Write.\\ 
    \hline
        Utilities & AOP loggers and HTTP helpers.\\ 
    \hline
    \caption{Modules available for unit testing}
    \label{table:unitTestCov} 
\end{longtable}

Out of the 28 different features of the given modules the unit tests were made for 26 of them. This gives a testing coverage of about
92\%. Some Test cases could not be carried out due to the complexity of making the tests i.e. AOP\footnote{AOP: Aspect Oriented Programming} loggers, HTTP helpers, File Writers.